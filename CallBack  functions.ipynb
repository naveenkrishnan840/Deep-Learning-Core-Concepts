{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Backs\n",
    "\n",
    "\n",
    "**Callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training.**\n",
    "\n",
    "\n",
    "*  Callbacks can help you prevent overfitting, visualize training progress, debug your code, save checkpoints, generate logs, create a TensorBoard, etc. \n",
    "\n",
    "\n",
    "\n",
    "* Note a couple of things here. First, callbacks are functions, which implies that you can roll your own if need be. Second, you can use more than one callback to monitor or affect the training of your model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### When a Callback is Triggered?\n",
    "\n",
    "\n",
    "Callbacks are called when a certain event is triggered. There are a few types of events during training that can lead to the trigger of a callback, such as:\n",
    "\n",
    "\n",
    "    (1) on_epoch_begin: as the name suggests, this event is triggered when a new epoch starts.\n",
    "    \n",
    "    (2) on_epoch_end: this is triggered when an epoch ends.\n",
    "    \n",
    "    (3) on_batch_begin: this is triggered when a new batch is passed for training.\n",
    "    \n",
    "    (4) on_batch_end: when a batch is finished with training.\n",
    "    \n",
    "    (5) on_train_begin: when the training starts.\n",
    "    \n",
    "    (6) on_train_end: when the training ends.\n",
    "    \n",
    "    \n",
    "## CallBacks API   \n",
    "\n",
    "\n",
    "### EarlyStopping:\n",
    "\n",
    "\n",
    "* This callback is used very often. This allows us to monitor our metrics, and stop model training when it stops improving. \n",
    "\n",
    "\n",
    "* For example, assume that you want to stop training if the accuracy is not improving by 0.05; you can use this callback to do so. This is useful in preventing overfitting of a model, to some extent.\n",
    "\n",
    "\n",
    "\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    "    )\n",
    "    \n",
    "    monitor: the names of the metrics we want to monitor.\n",
    "    \n",
    "    min_delta: the minimum amount of improvement we expect in every epoch.\n",
    "    \n",
    "    patience: the number of epochs to wait before stopping the training.\n",
    "    \n",
    "    verbose: whether or not to print additional logs.\n",
    "    \n",
    "    mode: defines whether the monitored metrics should be increasing, decreasing, or inferred from the name;      \n",
    "    possible values are 'min', 'max', or 'auto'.\n",
    "    \n",
    "    baseline: values for the monitored metrics.\n",
    "    \n",
    "    restore_best_weights: if set to True, the model will get the weights of the epoch which has the best value \n",
    "    for the monitored metrics; otherwise, it will get the weights of the last epoch.\n",
    "\n",
    "    The EarlyStopping callback is executed via the on_epoch_end trigger for training.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "### ModelCheckpoint:\n",
    "\n",
    "\n",
    "\n",
    "* This callback allows us to save the model regularly during training. \n",
    "\n",
    "\n",
    "* This is especially useful when training deep learning models which take a long time to train. This callback monitors the training and saves model checkpoints at regular intervals, based on the metrics.\n",
    "\n",
    "\n",
    "\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                     monitor='val_loss', \n",
    "                                     verbose=0, \n",
    "                                     save_best_only=False,\n",
    "                                     save_weights_only=False, \n",
    "                                     mode='auto', \n",
    "                                     save_freq='epoch')\n",
    "                                     \n",
    "    filepath: path for saving the model. You can pass the file path with formatting options like model- \n",
    "    {epoch:02d}-{val_loss:0.2f}; this saves the model with the mentioned values in the name.\n",
    "    \n",
    "    monitor: name of the metrics to monitor.\n",
    "    \n",
    "    save_best_only: if True, the best model will not be overridden.\n",
    "    \n",
    "    mode: defines whether the monitored metrics should be increasing, decreasing, or inferred from the name;\n",
    "    possible values are 'min', 'max', or 'auto'.\n",
    "    \n",
    "    save_weights_only: if True, only the weights of the models will be saved. Otherwise the full model will be\n",
    "    saved.\n",
    "    \n",
    "    save_freq: if 'epoch', the model will be saved after every epoch. If an integer value is passed, the model \n",
    "    will be saved after the integer number of batches (not to be confused with epochs).\n",
    "\n",
    "    The ModelCheckpoint callback is executed via the on_epoch_end trigger of training.\n",
    "    \n",
    "    \n",
    "    \n",
    "### TensorBoard:\n",
    "\n",
    "\n",
    "\n",
    "* This is one of the best callbacks if you want to visualize the training summary for your model. This callback generates the logs for TensorBoard, which you can later launch to visualize the progress of your training\n",
    "\n",
    "\n",
    "\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='logs',\n",
    "                                 histogram_freq=0, \n",
    "                                 write_graph=True, \n",
    "                                 write_images=False,    \n",
    "                                 update_freq='epoch', \n",
    "                                 profile_batch=2, \n",
    "                                 embeddings_freq=0,    \n",
    "                                 embeddings_metadata=None, \n",
    "                                 **kwargs)\n",
    "                                 \n",
    "    For now we will see only one parameter, log_dir, which is the path of the folder where you need to store the logs. To launch the TensorBoard you need to execute the following command:\n",
    "    tensorboard --logdir=path_to_your_logs     \n",
    "    The TensorBoard callback is also triggered at on_epoch_end\n",
    "    \n",
    "    \n",
    "    \n",
    "### LearningRateScheduler:\n",
    "\n",
    "\n",
    "\n",
    "* This callback is handy in scenarios where the user wants to update the learning rate as training progresses. For instance, as the training progresses you may want to decrease the learning rate after a certain number of epochs. The LearningRateScheduler will let you do exactly that.\n",
    "\n",
    "    tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)\n",
    "    \n",
    "    schedule: this is a function that takes the epoch index and returns a new learning rate.\n",
    "    \n",
    "    verbose: whether or not to print additional logs.\n",
    "    \n",
    "    This callback is also triggered at on_epoch_end\n",
    "    \n",
    "    \n",
    "    \n",
    "### CSVLogger:\n",
    "\n",
    "\n",
    "\n",
    "* As the name suggests, this callback logs the training details in a CSV file. The logged parameters are epoch, accuracy, loss, val_accuracy, and val_loss. One thing to keep in mind is that you need to pass accuracy as a metric while compiling the model, otherwise you will get an execution error.\n",
    "\n",
    "    tf.keras.callbacks.CSVLogger(filename, separator=',', append=False)\n",
    "                             \n",
    "                             \n",
    "* The logger accepts the filename, separator, and append as parameters. append defines whether or not to append to an existing file, or write in a new file instead.\n",
    "\n",
    "* The CSVLogger callback is executed via the on_epoch_end trigger of training. So when an epoch ends, the logs are put into a file.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_full,y_train_full),(x_test_full,y_test_full) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_val,x_train = x_train_full[0:4000]/255,x_train_full[4000:]/255\n",
    "y_train_val,y_train = y_train_full[0:4000],y_train_full[4000:]\n",
    "x_test_full = x_test_full/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7facf16f7430>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>0.650980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968627</td>\n",
       "      <td>0.498039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.674510</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.949020</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.250980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.219608</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.976471</td>\n",
       "      <td>0.250980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090196</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.674510</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3         4         5         6         7         8   \\\n",
       "0   0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.117647   \n",
       "7   0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.192157  0.933333   \n",
       "8   0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.070588  0.858824   \n",
       "9   0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.313725   \n",
       "10  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "17  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "20  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.090196   \n",
       "22  0.0  0.0  0.0  0.0  0.000000  0.000000  0.070588  0.670588  0.858824   \n",
       "23  0.0  0.0  0.0  0.0  0.215686  0.674510  0.886275  0.992157  0.992157   \n",
       "24  0.0  0.0  0.0  0.0  0.533333  0.992157  0.992157  0.992157  0.831373   \n",
       "25  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "26  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          9   ...        18        19        20        21        22        23  \\\n",
       "0   0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  ...  0.686275  0.101961  0.650980  1.000000  0.968627  0.498039   \n",
       "6   0.141176  ...  0.882353  0.674510  0.992157  0.949020  0.764706  0.250980   \n",
       "7   0.992157  ...  0.364706  0.321569  0.321569  0.219608  0.152941  0.000000   \n",
       "8   0.992157  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.611765  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.054902  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  ...  0.098039  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000  ...  0.588235  0.105882  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.000000  ...  0.992157  0.733333  0.000000  0.000000  0.000000  0.000000   \n",
       "17  0.000000  ...  0.992157  0.976471  0.250980  0.000000  0.000000  0.000000   \n",
       "18  0.000000  ...  0.992157  0.811765  0.007843  0.000000  0.000000  0.000000   \n",
       "19  0.000000  ...  0.980392  0.713725  0.000000  0.000000  0.000000  0.000000   \n",
       "20  0.000000  ...  0.305882  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21  0.258824  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "22  0.992157  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "23  0.992157  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "24  0.529412  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "25  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "26  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     24   25   26   27  \n",
       "0   0.0  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.0  \n",
       "5   0.0  0.0  0.0  0.0  \n",
       "6   0.0  0.0  0.0  0.0  \n",
       "7   0.0  0.0  0.0  0.0  \n",
       "8   0.0  0.0  0.0  0.0  \n",
       "9   0.0  0.0  0.0  0.0  \n",
       "10  0.0  0.0  0.0  0.0  \n",
       "11  0.0  0.0  0.0  0.0  \n",
       "12  0.0  0.0  0.0  0.0  \n",
       "13  0.0  0.0  0.0  0.0  \n",
       "14  0.0  0.0  0.0  0.0  \n",
       "15  0.0  0.0  0.0  0.0  \n",
       "16  0.0  0.0  0.0  0.0  \n",
       "17  0.0  0.0  0.0  0.0  \n",
       "18  0.0  0.0  0.0  0.0  \n",
       "19  0.0  0.0  0.0  0.0  \n",
       "20  0.0  0.0  0.0  0.0  \n",
       "21  0.0  0.0  0.0  0.0  \n",
       "22  0.0  0.0  0.0  0.0  \n",
       "23  0.0  0.0  0.0  0.0  \n",
       "24  0.0  0.0  0.0  0.0  \n",
       "25  0.0  0.0  0.0  0.0  \n",
       "26  0.0  0.0  0.0  0.0  \n",
       "27  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[28 rows x 28 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(200,activation=\"relu\",kernel_initializer=\"he_uniform\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(300,activation=\"relu\",kernel_initializer=\"he_uniform\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(200,activation=\"relu\",kernel_initializer=\"he_uniform\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(10,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\",loss=\"SparseCategoricalCrossentropy\",metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_cal_back = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=3,restore_best_weights=True)\n",
    "model_check_points = tf.keras.callbacks.ModelCheckpoint(filepath=\"modelcheckponits/\",monitor=\"accuracy\", save_best_only=True,\n",
    "                                               save_best_weights=True)\n",
    "\n",
    "def scheeduler(epoch,lr):\n",
    "    if epoch < 7:\n",
    "        return lr \n",
    "    else:\n",
    "        return lr * 0.99\n",
    "learn_change = tf.keras.callbacks.LearningRateScheduler(scheeduler, verbose=1)\n",
    "           \n",
    "csv_logger = tf.keras.callbacks.CSVLogger(\"csv_logs/csv_logs.csv\")\n",
    "csv_loggers = tf.keras.callbacks.CSVLogger(\"csv_logs/csv_logss.csv\")\n",
    "ACCURACY_THRESHOLD = 0.95\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "#     def on_epoch_end(self, epoch, logs=None): \n",
    "#         if(logs.get('accuracy') > ACCURACY_THRESHOLD):   \n",
    "#             print(f\"\\nReached {logs.get('accuracy')}'>'{ACCURACY_THRESHOLD*100} accuracy, so stopping training!!\")   \n",
    "#             self.model.stop_training = True\n",
    "    def on_train_batch_end(self, epoch, logs=None): \n",
    "        if(logs.get('accuracy') > ACCURACY_THRESHOLD):   \n",
    "            print(f\"\\nReached {logs.get('accuracy')}'>'{ACCURACY_THRESHOLD*100} accuracy, so stopping training!!\")   \n",
    "            self.model.stop_training = True        \n",
    "#     def on_train_batch_end(self, epoch, logs=None):\n",
    "#         print(epoch)\n",
    "#         print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "875/875 [==============================] - 24s 22ms/step - loss: 0.2212 - accuracy: 0.9319 - val_loss: 0.1193 - val_accuracy: 0.9675\n",
      "INFO:tensorflow:Assets written to: modelcheckponits/assets\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "875/875 [==============================] - 18s 21ms/step - loss: 0.0974 - accuracy: 0.9696 - val_loss: 0.0938 - val_accuracy: 0.9720\n",
      "INFO:tensorflow:Assets written to: modelcheckponits/assets\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "875/875 [==============================] - 16s 19ms/step - loss: 0.0698 - accuracy: 0.9777 - val_loss: 0.0887 - val_accuracy: 0.9750\n",
      "INFO:tensorflow:Assets written to: modelcheckponits/assets\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "875/875 [==============================] - 17s 19ms/step - loss: 0.0549 - accuracy: 0.9820 - val_loss: 0.0913 - val_accuracy: 0.9745\n",
      "INFO:tensorflow:Assets written to: modelcheckponits/assets\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "875/875 [==============================] - 17s 20ms/step - loss: 0.0461 - accuracy: 0.9846 - val_loss: 0.0847 - val_accuracy: 0.9765\n",
      "INFO:tensorflow:Assets written to: modelcheckponits/assets\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "875/875 [==============================] - 18s 20ms/step - loss: 0.0375 - accuracy: 0.9874 - val_loss: 0.0952 - val_accuracy: 0.9750\n",
      "INFO:tensorflow:Assets written to: modelcheckponits/assets\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "875/875 [==============================] - 17s 20ms/step - loss: 0.0324 - accuracy: 0.9887 - val_loss: 0.1001 - val_accuracy: 0.9755\n",
      "INFO:tensorflow:Assets written to: modelcheckponits/assets\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0009900000470224768.\n",
      "875/875 [==============================] - 18s 21ms/step - loss: 0.0306 - accuracy: 0.9896 - val_loss: 0.0976 - val_accuracy: 0.9732\n",
      "INFO:tensorflow:Assets written to: modelcheckponits/assets\n"
     ]
    }
   ],
   "source": [
    "auto_callbacks = model.fit(x_train,y_train,batch_size=64,epochs=100,validation_data=(x_train_val,y_train_val),callbacks=[early_stop_cal_back,model_check_points,learn_change, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "875/875 [==============================] - 19s 19ms/step - loss: 0.2191 - accuracy: 0.9321 - val_loss: 0.1136 - val_accuracy: 0.9672\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "  1/875 [..............................] - ETA: 17s - loss: 0.1386 - accuracy: 0.9531\n",
      "Reached 0.953125'>'95.0 accuracy, so stopping training!!\n",
      "875/875 [==============================] - 0s 467us/step - loss: 0.1386 - accuracy: 0.9531 - val_loss: 0.1135 - val_accuracy: 0.9668\n"
     ]
    }
   ],
   "source": [
    "custom_callback = model.fit(x_train,y_train,batch_size=64,epochs=100,validation_data=(x_train_val,y_train_val),callbacks=[learn_change, CustomCallback(), csv_loggers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "\n",
    "* Model_check_point callback is create one folder path and save model during training monitor based \n",
    "    on the metrics folder_name = modelcheckponits/\n",
    "    \n",
    "* This callbacks is reduce time to save model and each by each this callbacks is recording and save it    \n",
    "\n",
    "\n",
    "* EarlyStopping Callbacks allows monitors val_loss coming $8^{th}$ epoch model will be stopped during training And it lead prevent overfitting for saving model\n",
    "    \n",
    "    \n",
    "* Next i use custom callback is doing during on_train_batch_end one condition have accuracy> 0.95 ,So my\n",
    "    training will be stopped in particular epoch. $2^{nd}$ epochs model will be stopped during training\n",
    "    This custom callback also prevent overfitting model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
